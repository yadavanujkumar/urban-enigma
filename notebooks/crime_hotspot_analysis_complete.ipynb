{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Hotspot Prediction in India - Complete Analysis\n",
    "\n",
    "This notebook demonstrates the complete workflow for crime hotspot prediction and analysis using machine learning and data visualization techniques.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Preprocessing](#data-preprocessing)\n",
    "2. [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "3. [Hotspot Detection](#hotspot-detection)\n",
    "4. [Predictive Modeling](#predictive-modeling)\n",
    "5. [Time Series Forecasting](#time-series-forecasting)\n",
    "6. [Conclusions and Recommendations](#conclusions-and-recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import create_sample_crime_data, create_sample_demographic_data\n",
    "from src.data_preprocessing import CrimeDataPreprocessor\n",
    "from src.eda import CrimeDataAnalyzer\n",
    "from src.hotspot_detection import CrimeHotspotDetector\n",
    "from src.predictive_modeling import CrimePredictiveModel\n",
    "from src.time_series_forecasting import CrimeTimeSeriesForecaster\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First, let's create sample data and preprocess it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "print(\"Creating sample crime and demographic data...\")\n",
    "crime_data = create_sample_crime_data(1500)  # Create larger dataset for better analysis\n",
    "demographic_data = create_sample_demographic_data()\n",
    "\n",
    "print(f\"Crime data shape: {crime_data.shape}\")\n",
    "print(f\"Demographic data shape: {demographic_data.shape}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìä Sample Crime Data:\")\n",
    "display(crime_data.head())\n",
    "\n",
    "print(\"\\nüìä Sample Demographic Data:\")\n",
    "display(demographic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and clean the data\n",
    "preprocessor = CrimeDataPreprocessor()\n",
    "\n",
    "print(\"Starting data preprocessing pipeline...\")\n",
    "processed_data = preprocessor.preprocess_pipeline(crime_data, demographic_data)\n",
    "\n",
    "# Get preprocessing summary\n",
    "summary = preprocessor.get_preprocessing_summary(crime_data, processed_data)\n",
    "\n",
    "print(\"\\nüìà Preprocessing Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüìã Final processed data shape: {processed_data.shape}\")\n",
    "print(f\"üìã Total features: {processed_data.shape[1]}\")\n",
    "\n",
    "# Display processed data sample\n",
    "display(processed_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's explore the crime data to understand patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EDA analyzer\n",
    "analyzer = CrimeDataAnalyzer()\n",
    "\n",
    "# Get data overview\n",
    "overview = analyzer.get_data_overview(processed_data)\n",
    "\n",
    "print(\"üìä Data Overview:\")\n",
    "print(f\"  Shape: {overview['shape']}\")\n",
    "print(f\"  Memory Usage: {overview['memory_usage_mb']:.2f} MB\")\n",
    "print(f\"  Missing Values: {sum(overview['missing_values'].values())}\")\n",
    "print(f\"  Numerical Columns: {len(overview['numeric_columns'])}\")\n",
    "print(f\"  Categorical Columns: {len(overview['categorical_columns'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze crime trends by year\n",
    "analyzer.plot_crime_trends_by_year(processed_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze state-wise crime trends\n",
    "analyzer.plot_crime_trends_by_state(processed_data, top_n=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime intensity heatmap\n",
    "analyzer.plot_crime_intensity_heatmap(processed_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze crime categories\n",
    "crime_analysis = analyzer.analyze_crime_categories(processed_data)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Crime Category Analysis Results:\")\n",
    "print(f\"  Total Categories: {crime_analysis['total_categories']}\")\n",
    "print(f\"  Most Common Crime: {crime_analysis['most_common_crime']} ({crime_analysis['most_common_count']} cases)\")\n",
    "print(f\"  Least Common Crime: {crime_analysis['least_common_crime']} ({crime_analysis['least_common_count']} cases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal patterns analysis\n",
    "analyzer.plot_temporal_patterns(processed_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hotspot Detection\n",
    "\n",
    "Now let's identify crime hotspots using clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hotspot detector\n",
    "detector = CrimeHotspotDetector()\n",
    "\n",
    "# Prepare features for clustering\n",
    "features = detector.prepare_clustering_features(processed_data, \"location\")\n",
    "\n",
    "print(f\"üìç Prepared {features.shape[1]} features for clustering\")\n",
    "print(f\"üìç Using {features.shape[0]} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize K-Means clusters\n",
    "optimization_results = detector.optimize_kmeans_clusters(features, max_clusters=10)\n",
    "\n",
    "# Plot optimization results\n",
    "detector.plot_clustering_optimization(optimization_results)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Optimal number of clusters: {optimization_results['best_k_silhouette']}\")\n",
    "print(f\"üéØ Best silhouette score: {optimization_results['best_silhouette_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "optimal_k = optimization_results['best_k_silhouette']\n",
    "kmeans_data = detector.detect_hotspots(processed_data, method=\"kmeans\", \n",
    "                                       feature_set=\"location\", n_clusters=optimal_k)\n",
    "\n",
    "# Plot K-Means results\n",
    "detector.plot_hotspots_2d(kmeans_data, \"kmeans\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "dbscan_data = detector.detect_hotspots(processed_data, method=\"dbscan\", \n",
    "                                       feature_set=\"location\", eps=0.3, min_samples=10)\n",
    "\n",
    "# Plot DBSCAN results\n",
    "detector.plot_hotspots_2d(dbscan_data, \"dbscan\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hotspot characteristics\n",
    "kmeans_analysis = detector.analyze_hotspot_characteristics(kmeans_data, \"kmeans\")\n",
    "dbscan_analysis = detector.analyze_hotspot_characteristics(dbscan_data, \"dbscan\")\n",
    "\n",
    "print(\"üó∫Ô∏è K-Means Hotspot Analysis:\")\n",
    "print(f\"  Total Clusters: {kmeans_analysis['total_clusters']}\")\n",
    "print(f\"  Clustered Crimes: {kmeans_analysis['clustered_crimes']}/{kmeans_analysis['total_crimes']}\")\n",
    "\n",
    "if kmeans_analysis.get('largest_hotspot'):\n",
    "    largest = kmeans_analysis['largest_hotspot']\n",
    "    print(f\"  Largest Hotspot: {largest['most_affected_state']} ({largest['crime_count']} crimes)\")\n",
    "\n",
    "print(\"\\nüó∫Ô∏è DBSCAN Hotspot Analysis:\")\n",
    "print(f\"  Total Clusters: {dbscan_analysis['total_clusters']}\")\n",
    "print(f\"  Clustered Crimes: {dbscan_analysis['clustered_crimes']}/{dbscan_analysis['total_crimes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling\n",
    "\n",
    "Let's build machine learning models to predict crime types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize predictive model\n",
    "predictor = CrimePredictiveModel()\n",
    "\n",
    "# Prepare features and target\n",
    "X, y = predictor.prepare_features_target(processed_data)\n",
    "\n",
    "print(f\"üéØ Features prepared: {X.shape}\")\n",
    "print(f\"üéØ Target classes: {len(np.unique(y))}\")\n",
    "print(f\"üéØ Feature columns: {len(predictor.feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = predictor.split_data(X, y)\n",
    "\n",
    "print(f\"üìä Training set: {X_train.shape}\")\n",
    "print(f\"üìä Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = predictor.train_random_forest(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_metrics = predictor.evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Plot feature importance\n",
    "predictor.plot_feature_importance(rf_model, \"Random Forest\", top_n=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "xgb_model = predictor.train_xgboost(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_metrics = predictor.evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# Plot feature importance\n",
    "predictor.plot_feature_importance(xgb_model, \"XGBoost\", top_n=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "metrics_comparison = {'Random Forest': rf_metrics, 'XGBoost': xgb_metrics}\n",
    "predictor.plot_model_comparison(metrics_comparison)\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"üèÜ Model Performance Comparison:\")\n",
    "print(\"\\nRandom Forest:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "for metric, value in xgb_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample prediction\n",
    "sample_features = {\n",
    "    'latitude': 28.6139,  # Delhi coordinates\n",
    "    'longitude': 77.2090,\n",
    "    'month': 6,  # June\n",
    "    'weekday': 1,  # Tuesday\n",
    "    'population_normalized': 0.8,  # High population\n",
    "    'literacy_rate_normalized': 0.9,  # High literacy\n",
    "    'unemployment_rate_normalized': 0.3,  # Moderate unemployment\n",
    "    'urban_population_pct_normalized': 0.9  # Highly urban\n",
    "}\n",
    "\n",
    "# Predict with Random Forest\n",
    "rf_prediction = predictor.predict_crime_type(rf_model, sample_features, \"Random Forest\")\n",
    "\n",
    "print(\"üîÆ Sample Prediction (Delhi, June, Tuesday):\")\n",
    "print(f\"  Predicted Crime Type: {rf_prediction['predicted_crime_type']}\")\n",
    "print(f\"  Confidence: {rf_prediction['confidence']:.3f}\")\n",
    "\n",
    "if rf_prediction['probabilities']:\n",
    "    print(\"\\n  Top 3 Probabilities:\")\n",
    "    sorted_probs = sorted(rf_prediction['probabilities'].items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:3]\n",
    "    for crime_type, prob in sorted_probs:\n",
    "        print(f\"    {crime_type}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "Let's build time series models to forecast future crime trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize time series forecaster\n",
    "forecaster = CrimeTimeSeriesForecaster()\n",
    "\n",
    "# Prepare time series data\n",
    "time_series = forecaster.prepare_time_series(processed_data, freq=\"M\")\n",
    "\n",
    "print(f\"üìà Time series prepared: {len(time_series)} periods\")\n",
    "print(f\"üìà Date range: {time_series.index[0]} to {time_series.index[-1]}\")\n",
    "print(f\"üìà Mean crime count: {time_series.mean():.2f}\")\n",
    "\n",
    "# Display time series\n",
    "display(time_series.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time series characteristics\n",
    "ts_analysis = forecaster.analyze_time_series(time_series)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Time Series Analysis:\")\n",
    "for key, value in ts_analysis.items():\n",
    "    if key not in ['length']:\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split time series for training and testing\n",
    "split_point = int(len(time_series) * 0.8)\n",
    "ts_train = time_series[:split_point]\n",
    "ts_test = time_series[split_point:]\n",
    "\n",
    "print(f\"üìä Training periods: {len(ts_train)}\")\n",
    "print(f\"üìä Testing periods: {len(ts_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-select best ARIMA parameters\n",
    "best_order = forecaster.auto_arima_selection(ts_train, max_p=2, max_d=2, max_q=2)\n",
    "\n",
    "# Train ARIMA model\n",
    "arima_model = forecaster.train_arima_model(ts_train, best_order)\n",
    "\n",
    "# Generate ARIMA forecast\n",
    "arima_forecast, arima_conf = forecaster.forecast_arima(arima_model, len(ts_test))\n",
    "\n",
    "print(f\"üìà ARIMA{best_order} model trained\")\n",
    "print(f\"üìà Forecast generated for {len(arima_forecast)} periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LSTM data\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm, scaler = forecaster.prepare_lstm_data(\n",
    "    ts_train, lookback=6, test_size=0.2\n",
    ")\n",
    "\n",
    "if X_train_lstm is not None:\n",
    "    # Train LSTM model\n",
    "    lstm_model = forecaster.train_lstm_model(X_train_lstm, y_train_lstm, \n",
    "                                            X_test_lstm, y_test_lstm, epochs=30)\n",
    "    \n",
    "    # Generate LSTM forecast\n",
    "    last_sequence = X_train_lstm[-1].flatten()\n",
    "    lstm_forecast = forecaster.forecast_lstm(lstm_model, scaler, last_sequence, len(ts_test))\n",
    "    \n",
    "    print(f\"üìà LSTM model trained\")\n",
    "    print(f\"üìà LSTM forecast generated for {len(lstm_forecast)} periods\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient data for LSTM training\")\n",
    "    lstm_forecast = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecasts comparison\n",
    "forecaster.plot_forecasts()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate forecasts\n",
    "if 'arima' in forecaster.forecasts:\n",
    "    arima_metrics = forecaster.evaluate_forecasts(ts_test, arima_forecast, \"ARIMA\")\n",
    "    print(\"\\nüìä ARIMA Forecast Evaluation:\")\n",
    "    for metric, value in arima_metrics.items():\n",
    "        print(f\"  {metric}: {value:.2f}\")\n",
    "\n",
    "if lstm_forecast is not None:\n",
    "    lstm_metrics = forecaster.evaluate_forecasts(ts_test, lstm_forecast, \"LSTM\")\n",
    "    print(\"\\nüìä LSTM Forecast Evaluation:\")\n",
    "    for metric, value in lstm_metrics.items():\n",
    "        print(f\"  {metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "\n",
    "Based on our comprehensive analysis, here are the key findings and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights\n",
    "print(\"üéØ KEY FINDINGS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìä Data Analysis Insights:\")\n",
    "print(f\"  ‚Ä¢ Analyzed {processed_data.shape[0]:,} crime records across {processed_data['state'].nunique()} states\")\n",
    "print(f\"  ‚Ä¢ Most common crime type: {crime_analysis['most_common_crime']}\")\n",
    "print(f\"  ‚Ä¢ Geographic coverage: {processed_data['district'].nunique()} districts\")\n",
    "\n",
    "print(\"\\nüó∫Ô∏è Hotspot Detection Results:\")\n",
    "print(f\"  ‚Ä¢ K-Means identified {kmeans_analysis['total_clusters']} distinct hotspots\")\n",
    "print(f\"  ‚Ä¢ DBSCAN found {dbscan_analysis['total_clusters']} dense crime clusters\")\n",
    "if kmeans_analysis.get('largest_hotspot'):\n",
    "    largest = kmeans_analysis['largest_hotspot']\n",
    "    print(f\"  ‚Ä¢ Priority hotspot: {largest['most_affected_state']} ({largest['crime_count']} crimes)\")\n",
    "\n",
    "print(\"\\nü§ñ Predictive Modeling Performance:\")\n",
    "print(f\"  ‚Ä¢ Random Forest Accuracy: {rf_metrics['accuracy']:.3f}\")\n",
    "print(f\"  ‚Ä¢ XGBoost Accuracy: {xgb_metrics['accuracy']:.3f}\")\n",
    "best_model = \"Random Forest\" if rf_metrics['f1_weighted'] > xgb_metrics['f1_weighted'] else \"XGBoost\"\n",
    "print(f\"  ‚Ä¢ Best performing model: {best_model}\")\n",
    "\n",
    "print(\"\\nüìà Time Series Forecasting:\")\n",
    "print(f\"  ‚Ä¢ Analyzed {len(time_series)} months of crime data\")\n",
    "print(f\"  ‚Ä¢ Seasonal patterns detected: {ts_analysis.get('seasonality_detected', False)}\")\n",
    "print(f\"  ‚Ä¢ Best ARIMA model: ARIMA{best_order}\")\n",
    "\n",
    "print(\"\\nüéØ STRATEGIC RECOMMENDATIONS:\")\n",
    "print(\"  1. Deploy additional police patrols in identified K-Means hotspots\")\n",
    "print(\"  2. Implement targeted crime prevention programs in high-density areas\")\n",
    "print(f\"  3. Focus on {crime_analysis['most_common_crime']} prevention strategies\")\n",
    "print(\"  4. Use machine learning models for predictive policing\")\n",
    "print(\"  5. Monitor seasonal crime patterns for resource allocation\")\n",
    "print(\"  6. Integrate demographic factors in crime prevention planning\")\n",
    "print(\"  7. Establish real-time monitoring systems in top hotspots\")\n",
    "print(\"  8. Regular model retraining with new crime data\")\n",
    "\n",
    "print(\"\\n‚úÖ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"All models, visualizations, and insights are ready for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Deploy the Streamlit Dashboard**: Run `streamlit run streamlit_app/app.py` to access the interactive dashboard\n",
    "2. **Model Deployment**: Use the saved models for real-time crime prediction\n",
    "3. **Data Integration**: Connect with real crime databases for live analysis\n",
    "4. **Continuous Monitoring**: Set up automated model retraining pipelines\n",
    "5. **Stakeholder Engagement**: Share insights with law enforcement agencies\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis demonstrates a complete end-to-end machine learning pipeline for crime hotspot prediction and analysis. The models and visualizations can be adapted for different regions and crime datasets.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}