{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Hotspot Prediction in India - Complete Analysis\n",
    "\n",
    "This notebook demonstrates the complete workflow for crime hotspot prediction and analysis using machine learning and data visualization techniques.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Preprocessing](#data-preprocessing)\n",
    "2. [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "3. [Hotspot Detection](#hotspot-detection)\n",
    "4. [Predictive Modeling](#predictive-modeling)\n",
    "5. [Time Series Forecasting](#time-series-forecasting)\n",
    "6. [Conclusions and Recommendations](#conclusions-and-recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import create_sample_crime_data, create_sample_demographic_data\n",
    "from src.data_preprocessing import CrimeDataPreprocessor\n",
    "from src.eda import CrimeDataAnalyzer\n",
    "from src.hotspot_detection import CrimeHotspotDetector\n",
    "from src.predictive_modeling import CrimePredictiveModel\n",
    "from src.time_series_forecasting import CrimeTimeSeriesForecaster\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First, let's create sample data and preprocess it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets\n",
    "print(\"Creating sample crime and demographic data...\")\n",
    "crime_data = create_sample_crime_data(1500)  # Create larger dataset for better analysis\n",
    "demographic_data = create_sample_demographic_data()\n",
    "\n",
    "print(f\"Crime data shape: {crime_data.shape}\")\n",
    "print(f\"Demographic data shape: {demographic_data.shape}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nðŸ“Š Sample Crime Data:\")\n",
    "display(crime_data.head())\n",
    "\n",
    "print(\"\\nðŸ“Š Sample Demographic Data:\")\n",
    "display(demographic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and clean the data\n",
    "preprocessor = CrimeDataPreprocessor()\n",
    "\n",
    "print(\"Starting data preprocessing pipeline...\")\n",
    "processed_data = preprocessor.preprocess_pipeline(crime_data, demographic_data)\n",
    "\n",
    "# Get preprocessing summary\n",
    "summary = preprocessor.get_preprocessing_summary(crime_data, processed_data)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Preprocessing Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Final processed data shape: {processed_data.shape}\")\n",
    "print(f\"ðŸ“‹ Total features: {processed_data.shape[1]}\")\n",
    "\n",
    "# Display processed data sample\n",
    "display(processed_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's explore the crime data to understand patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EDA analyzer\n",
    "analyzer = CrimeDataAnalyzer()\n",
    "\n",
    "# Get data overview\n",
    "overview = analyzer.get_data_overview(processed_data)\n",
    "\n",
    "print(\"ðŸ“Š Data Overview:\")\n",
    "print(f\"  Shape: {overview['shape']}\")\n",
    "print(f\"  Memory Usage: {overview['memory_usage_mb']:.2f} MB\")\n",
    "print(f\"  Missing Values: {sum(overview['missing_values'].values())}\")\n",
    "print(f\"  Numerical Columns: {len(overview['numeric_columns'])}\")\n",
    "print(f\"  Categorical Columns: {len(overview['categorical_columns'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze crime trends by year\n",
    "analyzer.plot_crime_trends_by_year(processed_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze state-wise crime trends\n",
    "analyzer.plot_crime_trends_by_state(processed_data, top_n=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime intensity heatmap\n",
    "analyzer.plot_crime_intensity_heatmap(processed_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze crime categories\n",
    "crime_analysis = analyzer.analyze_crime_categories(processed_data)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Crime Category Analysis Results:\")\n",
    "print(f\"  Total Categories: {crime_analysis['total_categories']}\")\n",
    "print(f\"  Most Common Crime: {crime_analysis['most_common_crime']} ({crime_analysis['most_common_count']} cases)\")\n",
    "print(f\"  Least Common Crime: {crime_analysis['least_common_crime']} ({crime_analysis['least_common_count']} cases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal patterns analysis\n",
    "analyzer.plot_temporal_patterns(processed_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hotspot Detection\n",
    "\n",
    "Now let's identify crime hotspots using clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hotspot detector\n",
    "detector = CrimeHotspotDetector()\n",
    "\n",
    "# Prepare features for clustering\n",
    "features = detector.prepare_clustering_features(processed_data, \"location\")\n",
    "\n",
    "print(f\"ðŸ“ Prepared {features.shape[1]} features for clustering\")\n",
    "print(f\"ðŸ“ Using {features.shape[0]} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize K-Means clusters\n",
    "optimization_results = detector.optimize_kmeans_clusters(features, max_clusters=10)\n",
    "\n",
    "# Plot optimization results\n",
    "detector.plot_clustering_optimization(optimization_results)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Optimal number of clusters: {optimization_results['best_k_silhouette']}\")\n",
    "print(f\"ðŸŽ¯ Best silhouette score: {optimization_results['best_silhouette_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "optimal_k = optimization_results['best_k_silhouette']\n",
    "kmeans_data = detector.detect_hotspots(processed_data, method=\"kmeans\", \n",
    "                                       feature_set=\"location\", n_clusters=optimal_k)\n",
    "\n",
    "# Plot K-Means results\n",
    "detector.plot_hotspots_2d(kmeans_data, \"kmeans\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "dbscan_data = detector.detect_hotspots(processed_data, method=\"dbscan\", \n",
    "                                       feature_set=\"location\", eps=0.3, min_samples=10)\n",
    "\n",
    "# Plot DBSCAN results\n",
    "detector.plot_hotspots_2d(dbscan_data, \"dbscan\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hotspot characteristics\n",
    "kmeans_analysis = detector.analyze_hotspot_characteristics(kmeans_data, \"kmeans\")\n",
    "dbscan_analysis = detector.analyze_hotspot_characteristics(dbscan_data, \"dbscan\")\n",
    "\n",
    "print(\"ðŸ—ºï¸ K-Means Hotspot Analysis:\")\n",
    "print(f\"  Total Clusters: {kmeans_analysis['total_clusters']}\")\n",
    "print(f\"  Clustered Crimes: {kmeans_analysis['clustered_crimes']}/{kmeans_analysis['total_crimes']}\")\n",
    "\n",
    "if kmeans_analysis.get('largest_hotspot'):\n",
    "    largest = kmeans_analysis['largest_hotspot']\n",
    "    print(f\"  Largest Hotspot: {largest['most_affected_state']} ({largest['crime_count']} crimes)\")\n",
    "\n",
    "print(\"\\nðŸ—ºï¸ DBSCAN Hotspot Analysis:\")\n",
    "print(f\"  Total Clusters: {dbscan_analysis['total_clusters']}\")\n",
    "print(f\"  Clustered Crimes: {dbscan_analysis['clustered_crimes']}/{dbscan_analysis['total_crimes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling\n",
    "\n",
    "Let's build machine learning models to predict crime types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize predictive model\n",
    "predictor = CrimePredictiveModel()\n",
    "\n",
    "# Prepare features and target\n",
    "X, y = predictor.prepare_features_target(processed_data)\n",
    "\n",
    "print(f\"ðŸŽ¯ Features prepared: {X.shape}\")\n",
    "print(f\"ðŸŽ¯ Target classes: {len(np.unique(y))}\")\n",
    "print(f\"ðŸŽ¯ Feature columns: {len(predictor.feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = predictor.split_data(X, y)\n",
    "\n",
    "print(f\"ðŸ“Š Training set: {X_train.shape}\")\n",
    "print(f\"ðŸ“Š Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = predictor.train_random_forest(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_metrics = predictor.evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Plot feature importance\n",
    "predictor.plot_feature_importance(rf_model, \"Random Forest\", top_n=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "xgb_model = predictor.train_xgboost(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_metrics = predictor.evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# Plot feature importance\n",
    "predictor.plot_feature_importance(xgb_model, \"XGBoost\", top_n=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "metrics_comparison = {'Random Forest': rf_metrics, 'XGBoost': xgb_metrics}\n",
    "predictor.plot_model_comparison(metrics_comparison)\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"ðŸ† Model Performance Comparison:\")\n",
    "print(\"\\nRandom Forest:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "for metric, value in xgb_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample prediction\n",
    "sample_features = {\n",
    "    'latitude': 28.6139,  # Delhi coordinates\n",
    "    'longitude': 77.2090,\n",
    "    'month': 6,  # June\n",
    "    'weekday': 1,  # Tuesday\n",
    "    'population_normalized': 0.8,  # High population\n",
    "    'literacy_rate_normalized': 0.9,  # High literacy\n",
    "    'unemployment_rate_normalized': 0.3,  # Moderate unemployment\n",
    "    'urban_population_pct_normalized': 0.9  # Highly urban\n",
    "}\n",
    "\n",
    "# Predict with Random Forest\n",
    "rf_prediction = predictor.predict_crime_type(rf_model, sample_features, \"Random Forest\")\n",
    "\n",
    "print(\"ðŸ”® Sample Prediction (Delhi, June, Tuesday):\")\n",
    "print(f\"  Predicted Crime Type: {rf_prediction['predicted_crime_type']}\")\n",
    "print(f\"  Confidence: {rf_prediction['confidence']:.3f}\")\n",
    "\n",
    "if rf_prediction['probabilities']:\n",
    "    print(\"\\n  Top 3 Probabilities:\")\n",
    "    sorted_probs = sorted(rf_prediction['probabilities'].items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:3]\n",
    "    for crime_type, prob in sorted_probs:\n",
    "        print(f\"    {crime_type}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "Let's build time series models to forecast future crime trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize time series forecaster\n",
    "forecaster = CrimeTimeSeriesForecaster()\n",
    "\n",
    "# Prepare time series data\n",
    "time_series = forecaster.prepare_time_series(processed_data, freq=\"M\")\n",
    "\n",
    "print(f\"ðŸ“ˆ Time series prepared: {len(time_series)} periods\")\n",
    "print(f\"ðŸ“ˆ Date range: {time_series.index[0]} to {time_series.index[-1]}\")\n",
    "print(f\"ðŸ“ˆ Mean crime count: {time_series.mean():.2f}\")\n",
    "\n",
    "# Display time series\n",
    "display(time_series.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time series characteristics\n",
    "ts_analysis = forecaster.analyze_time_series(time_series)\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Time Series Analysis:\")\n",
    "for key, value in ts_analysis.items():\n",
    "    if key not in ['length']:\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split time series for training and testing\n",
    "split_point = int(len(time_series) * 0.8)\n",
    "ts_train = time_series[:split_point]\n",
    "ts_test = time_series[split_point:]\n",
    "\n",
    "print(f\"ðŸ“Š Training periods: {len(ts_train)}\")\n",
    "print(f\"ðŸ“Š Testing periods: {len(ts_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-select best ARIMA parameters\n",
    "best_order = forecaster.auto_arima_selection(ts_train, max_p=2, max_d=2, max_q=2)\n",
    "\n",
    "# Train ARIMA model\n",
    "arima_model = forecaster.train_arima_model(ts_train, best_order)\n",
    "\n",
    "# Generate ARIMA forecast\n",
    "arima_forecast, arima_conf = forecaster.forecast_arima(arima_model, len(ts_test))\n",
    "\n",
    "print(f\"ðŸ“ˆ ARIMA{best_order} model trained\")\n",
    "print(f\"ðŸ“ˆ Forecast generated for {len(arima_forecast)} periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LSTM data\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm, scaler = forecaster.prepare_lstm_data(\n",
    "    ts_train, lookback=6, test_size=0.2\n",
    ")\n",
    "\n",
    "if X_train_lstm is not None:\n",
    "    # Train LSTM model\n",
    "    lstm_model = forecaster.train_lstm_model(X_train_lstm, y_train_lstm, \n",
    "                                            X_test_lstm, y_test_lstm, epochs=30)\n",
    "    \n",
    "    # Generate LSTM forecast\n",
    "    last_sequence = X_train_lstm[-1].flatten()\n",
    "    lstm_forecast = forecaster.forecast_lstm(lstm_model, scaler, last_sequence, len(ts_test))\n",
    "    \n",
    "    print(f\"ðŸ“ˆ LSTM model trained\")\n",
    "    print(f\"ðŸ“ˆ LSTM forecast generated for {len(lstm_forecast)} periods\")\n",
    "else:\n",
    "    print(\"âš ï¸ Insufficient data for LSTM training\")\n",
    "    lstm_forecast = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecasts comparison\n",
    "forecaster.plot_forecasts()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate forecasts\n",
    "if 'arima' in forecaster.forecasts:\n",
    "    arima_metrics = forecaster.evaluate_forecasts(ts_test, arima_forecast, \"ARIMA\")\n",
    "    print(\"\\nðŸ“Š ARIMA Forecast Evaluation:\")\n",
    "    for metric, value in arima_metrics.items():\n",
    "        print(f\"  {metric}: {value:.2f}\")\n",
    "\n",
    "if lstm_forecast is not None:\n",
    "    lstm_metrics = forecaster.evaluate_forecasts(ts_test, lstm_forecast, \"LSTM\")\n",
    "    print(\"\\nðŸ“Š LSTM Forecast Evaluation:\")\n",
    "    for metric, value in lstm_metrics.items():\n",
    "        print(f\"  {metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "\n",
    "Based on our comprehensive analysis, here are the key findings and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights\n",
    "print(\"ðŸŽ¯ KEY FINDINGS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nðŸ“Š Data Analysis Insights:\")\n",
    "print(f\"  â€¢ Analyzed {processed_data.shape[0]:,} crime records across {processed_data['state'].nunique()} states\")\n",
    "print(f\"  â€¢ Most common crime type: {crime_analysis['most_common_crime']}\")\n",
    "print(f\"  â€¢ Geographic coverage: {processed_data['district'].nunique()} districts\")\n",
    "\n",
    "print(\"\\nðŸ—ºï¸ Hotspot Detection Results:\")\n",
    "print(f\"  â€¢ K-Means identified {kmeans_analysis['total_clusters']} distinct hotspots\")\n",
    "print(f\"  â€¢ DBSCAN found {dbscan_analysis['total_clusters']} dense crime clusters\")\n",
    "if kmeans_analysis.get('largest_hotspot'):\n",
    "    largest = kmeans_analysis['largest_hotspot']\n",
    "    print(f\"  â€¢ Priority hotspot: {largest['most_affected_state']} ({largest['crime_count']} crimes)\")\n",
    "\n",
    "print(\"\\nðŸ¤– Predictive Modeling Performance:\")\n",
    "print(f\"  â€¢ Random Forest Accuracy: {rf_metrics['accuracy']:.3f}\")\n",
    "print(f\"  â€¢ XGBoost Accuracy: {xgb_metrics['accuracy']:.3f}\")\n",
    "best_model = \"Random Forest\" if rf_metrics['f1_weighted'] > xgb_metrics['f1_weighted'] else \"XGBoost\"\n",
    "print(f\"  â€¢ Best performing model: {best_model}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Time Series Forecasting:\")\n",
    "print(f\"  â€¢ Analyzed {len(time_series)} months of crime data\")\n",
    "print(f\"  â€¢ Seasonal patterns detected: {ts_analysis.get('seasonality_detected', False)}\")\n",
    "print(f\"  â€¢ Best ARIMA model: ARIMA{best_order}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ STRATEGIC RECOMMENDATIONS:\")\n",
    "print(\"  1. Deploy additional police patrols in identified K-Means hotspots\")\n",
    "print(\"  2. Implement targeted crime prevention programs in high-density areas\")\n",
    "print(f\"  3. Focus on {crime_analysis['most_common_crime']} prevention strategies\")\n",
    "print(\"  4. Use machine learning models for predictive policing\")\n",
    "print(\"  5. Monitor seasonal crime patterns for resource allocation\")\n",
    "print(\"  6. Integrate demographic factors in crime prevention planning\")\n",
    "print(\"  7. Establish real-time monitoring systems in top hotspots\")\n",
    "print(\"  8. Regular model retraining with new crime data\")\n",
    "\n",
    "print(\"\\nâœ… ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"All models, visualizations, and insights are ready for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Deploy the Streamlit Dashboard**: Run `streamlit run streamlit_app/app.py` to access the interactive dashboard\n",
    "2. **Model Deployment**: Use the saved models for real-time crime prediction\n",
    "3. **Data Integration**: Connect with real crime databases for live analysis\n",
    "4. **Continuous Monitoring**: Set up automated model retraining pipelines\n",
    "5. **Stakeholder Engagement**: Share insights with law enforcement agencies\n",
    "\n",
    "---\n",
    "\n",
    "*This analysis demonstrates a complete end-to-end machine learning pipeline for crime hotspot prediction and analysis. The models and visualizations can be adapted for different regions and crime datasets.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}